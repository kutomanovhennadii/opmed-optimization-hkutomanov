\# ðŸ§© T2.2.2 â€” Theoretical Background for Hyperparameter Selection and Solver Tuning



\## 1. Purpose

To provide a theoretical framework for understanding and selecting \*\*hyperparameters\*\* of the CP-SAT solver and hybrid optimization pipeline.  

The objective is to balance between \*\*solution optimality\*\*, \*\*runtime\*\*, and \*\*reproducibility\*\*, grounded in the principles of combinatorial search and constraint propagation.



---



\## 2. Nature of CP-SAT Hyperparameters

Unlike traditional ML hyperparameters, CP-SAT parameters control \*\*search-space exploration\*\* and \*\*propagation behavior\*\* rather than statistical learning.  

They influence:

\- The \*\*order\*\* in which decisions are made (branching strategy);  

\- The \*\*aggressiveness\*\* of constraint propagation;  

\- The \*\*scope\*\* of linearization and relaxation;  

\- The \*\*balance\*\* between deterministic and stochastic search components.



---



\## 3. Major Hyperparameters and Their Theoretical Roles



| Parameter | Type | Theoretical Role | Effect on Search Behavior |

|------------|------|------------------|----------------------------|

| `max\_time\_in\_seconds` | Integer | Defines time budget (T\_limit) for bounded optimization | Higher value â†’ better solution quality, slower runtime |

| `num\_search\_workers` | Integer | Parallel search threads exploring disjoint subspaces | Increases exploration breadth, may reduce determinism |

| `search\_branching` | Enum(`AUTOMATIC`, `FIXED`, `LP\_SEARCH`) | Controls branching heuristics | `AUTOMATIC` adapts dynamically to constraint density |

| `linearization\_level` | Integer (0â€“3) | Defines strength of linear relaxation | Higher levels increase pruning power, but cost more memory |

| `random\_seed` | Integer | Ensures reproducibility of randomized components | Identical seeds â†’ identical deterministic results |

| `cp\_model\_presolve` | Bool | Enables constraint simplification before solving | Reduces redundant constraints, speeds up propagation |

| `log\_search\_progress` | Bool | Enables intermediate solution logging | Adds traceability at slight runtime cost |

| `max\_memory\_in\_mb` | Integer | Memory cap for large-scale runs | Prevents overflow in massive instance exploration |



These hyperparameters collectively determine the \*\*explorationâ€“exploitation trade-off\*\* in the solver.



---



\## 4. Theoretical Foundations of Tuning



\### 4.1. Search Space Structure

The total number of feasible assignments is exponential in N (number of surgeries).  

Effective parameter tuning aims to reduce \*\*search tree depth\*\* by:

\- Guiding branching order toward high-impact variables (interval start times, room assignments).  

\- Increasing propagation frequency to prune infeasible subtrees early.  

\- Enabling presolve to collapse redundant linear constraints.



\### 4.2. Propagation vs. Linearization

\- \*\*Propagation strength\*\* correlates with domain filtering (narrowing feasible variable intervals).  

\- \*\*Linearization strength\*\* (controlled by `linearization\_level`) enhances relaxation bounds and improves lower-bound estimates.  

\- A higher linearization level improves optimality gap closure but increases memory consumption and solver latency.



\### 4.3. Parallelism and Stochasticity

\- Each thread in CP-SAT runs an independent search strategy seeded differently.  

\- Increasing `num\_search\_workers` introduces \*\*diversified local searches\*\*, approximating a \*\*cooperative metaheuristic\*\*.  

\- However, reproducibility decreases when `random\_seed` is not fixed.



\### 4.4. Time-Bounded Optimization

\- For large N, it is computationally infeasible to guarantee global optimality.  

\- CP-SAT employs \*\*anytime optimization\*\* â€” it produces the best feasible solution found within the time budget.  

\- The trade-off curve between cost and time follows an empirical law:  

&nbsp; Improvement ~ 1 / log(T\_limit)



This means that doubling runtime yields diminishing returns in solution quality.



---



\## 5. Recommended Hyperparameter Strategy



| Scenario | Objective | Recommended Configuration |

|-----------|------------|----------------------------|

| \*\*Small instance (N < 100)\*\* | Find exact optimum | `max\_time\_in\_seconds=120`, `search\_branching=AUTOMATIC`, `num\_search\_workers=1` |

| \*\*Medium instance (N = 100â€“1000)\*\* | Near-optimal within time limit | `max\_time\_in\_seconds=300â€“600`, `num\_search\_workers=4â€“8`, `linearization\_level=1â€“2` |

| \*\*Large instance (N > 1000)\*\* | Fast feasible schedule | `max\_time\_in\_seconds=120â€“300`, `num\_search\_workersâ‰¥8`, `linearization\_level=0`, enable presolve |

| \*\*MLOps experimentation\*\* | Sensitivity analysis | Sweep over `search\_branching` and `linearization\_level` with fixed seed grid |



---



\## 6. MLOps Integration for Hyperparameter Tuning

Hyperparameter tuning can be automated using MLOps workflows:

1\. \*\*Experiment Orchestration:\*\* Airflow DAG runs multiple solver configurations.  

2\. \*\*Tracking:\*\* MLflow logs metrics (`cost`, `utilization`, `runtime`, `gap`).  

3\. \*\*Selection:\*\* Bayesian or grid search selects Pareto-optimal trade-offs.  

4\. \*\*Visualization:\*\* Neptune.ai or Weights \& Biases dashboards display performance surfaces.  



Each solver run is treated as a reproducible experiment identified by (seed, parameters, timestamp).



---



\## 7. Theoretical Metrics for Evaluation

To evaluate solver configurations theoretically:



| Metric | Formula | Interpretation |

|---------|----------|----------------|

| \*\*Feasibility Rate (F)\*\* | (# valid constraints) / (total constraints) | Proportion of valid constraint satisfaction |

| \*\*Optimality Gap (G)\*\* | (C\_found âˆ’ C\_best) / C\_best | Quality gap vs. best known bound |

| \*\*Stability (S)\*\* | Var(C\_found over seeds) | Sensitivity to stochastic initialization |

| \*\*Runtime Efficiency (E)\*\* | (Constraint checks / second) | Internal propagation speed indicator |



A good configuration achieves low G, high F, and low S within runtime bounds.



---



\## 8. Conclusion

CP-SATâ€™s hyperparameters directly govern the efficiency and consistency of the optimization process.  

Their tuning is a \*\*mathematical calibration problem\*\*, not empirical guessing.  

By controlling branching, propagation, and randomization, we can achieve predictable, reproducible trade-offs between runtime and optimality.  

When integrated with MLOps tools, this tuning process becomes a continuous experiment pipeline, providing data-driven performance refinement.



---



\*\*Status:\*\* Theoretical tuning framework complete  

\*\*Dependencies:\*\* T2.2.1 (solver comparison), T2.1.4 (method justification)  

