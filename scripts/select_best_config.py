# scripts/select_best_config.py
"""
Selects the best configuration from a tuning run.

This script implements Epic 7, Task 7.3.3.
1. Loads 'configs/tune_grid.yaml' to find the optimization objectives.
2. Loads 'data/output/tune/tuning_results.csv' into pandas.
3. Filters out failed runs (keeps only FEASIBLE/OPTIMAL).
4. Sorts the DataFrame by the primary and secondary objectives.
5. Takes the top row (the "best run").
6. Loads the base 'configs/config.yaml'.
7. Overwrites the base config with the parameters from the best run.
8. Saves the result as 'configs/best_config.yaml'.
9. Appends a summary table to 'docs/RESULTS_tuning.md'.
"""

import logging
import sys
from pathlib import Path
from typing import Any

try:
    import pandas as pd
    import yaml
except ImportError as e:
    print(
        f"Error: Missing dependencies. Please install 'pandas' and 'pyyaml'.\n"
        f"(e.g., 'poetry add pandas pyyaml' or 'pip install pandas pyyaml')\n"
        f"Details: {e}",
        file=sys.stderr,
    )
    sys.exit(1)


# --- Constants ---
BASE_CONFIG_PATH = Path("configs/config.yaml")
TUNE_GRID_PATH = Path("configs/tune_grid.yaml")
BEST_CONFIG_PATH = Path("configs/best_config.yaml")

# This is the output CSV from tune.py
RESULTS_CSV_PATH = Path("data/output/tune/tuning_results.csv")
# This is the Markdown report file from the DoD
RESULTS_DOC_PATH = Path("docs/RESULTS_tuning.md")

# Columns from the CSV that are *metrics*, not *parameters*
METRIC_COLUMNS = [
    "run_id",
    "dataset",
    "status",
    "utilization",
    "runtime_sec",
    "total_cost",
    "num_anesthetists",
    "num_rooms_used",
    "is_valid",
]


def setup_logging() -> None:
    """Configures basic logging to stdout."""
    logging.basicConfig(
        level=logging.INFO,
        format="[%(asctime)s] [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def load_yaml_config(path: Path) -> dict[str, Any]:
    """Loads a YAML file as a dictionary."""
    if not path.exists():
        logging.error(f"File not found: {path}")
        raise FileNotFoundError(f"File not found: {path}")
    try:
        return yaml.safe_load(path.read_text(encoding="utf-8"))
    except yaml.YAMLError as e:
        logging.error(f"Error parsing YAML in {path}: {e}")
        raise


def set_nested_key(config_dict: dict, key: str, value: Any) -> None:
    """
    Sets a nested key in a dictionary by a 'a.b.c' string.
    Example: set_nested_key(cfg, 'solver.num_workers', 8)
    -> cfg['solver']['num_workers'] = 8
    """
    keys = key.split(".")
    d = config_dict
    for k in keys[:-1]:
        d = d.setdefault(k, {})
    d[keys[-1]] = value


def save_best_config(base_cfg: dict, best_params: dict) -> None:
    """
    Applies the best parameters to the base config and saves it.
    """
    # We modify the base config in place
    config_to_save = base_cfg

    logging.info("Applying best parameters to base config...")
    for key, value in best_params.items():
        # Check for 'nan' from pandas, which is not valid YAML
        if pd.isna(value):
            logging.warning(f"  Skipping param {key}: value is NaN")
            continue

        # Convert numpy types (like int64) to standard python types
        if hasattr(value, "item"):
            value = value.item()

        logging.info(f"  -> {key} = {value}")
        set_nested_key(config_to_save, key, value)

    # Save the new config
    try:
        with BEST_CONFIG_PATH.open("w", encoding="utf-8") as f:
            yaml.safe_dump(config_to_save, f, default_flow_style=False, sort_keys=False)
        logging.info(f"Successfully saved: {BEST_CONFIG_PATH}")
    except Exception as e:
        logging.error(f"Failed to write {BEST_CONFIG_PATH}: {e}")
        raise


def update_markdown_report(df_top5: pd.DataFrame, best_run: pd.Series) -> None:
    """Appends the summary table to the docs file."""
    logging.info(f"Updating report: {RESULTS_DOC_PATH}")

    # Define which columns to show in the summary
    summary_cols = [
        "dataset",
        "utilization",
        "runtime_sec",
        "total_cost",
        "status",
        "activation_penalty",
        "solver.search_branching",
        "solver.stop_after_first_solution",
    ]
    # Filter df_top5 to only columns that exist
    display_cols = [col for col in summary_cols if col in df_top5.columns]
    report_table = df_top5[display_cols].to_markdown(index=False, floatfmt=".4f")

    report_content = f"""
---
## Tuning Run Results (Auto-generated by select_best_config.py)

Run performed at: {pd.Timestamp.now().isoformat()}
Results CSV: `{RESULTS_CSV_PATH.name}`

### Best Configuration Found
The best configuration (based on aggregated results) has been saved to:
`{BEST_CONFIG_PATH.name}`

Parameters from best run (ID: `{best_run['run_id']}`):
* **Dataset:** `{best_run['dataset']}`
* **Utilization:** `{best_run['utilization']:.4f}`
* **Runtime (sec):** `{best_run['runtime_sec']:.2f}`
* **Total Cost:** `{best_run['total_cost']}`
* **Params:**
    * `activation_penalty`: `{best_run.get('activation_penalty', 'N/A')}`
    * `solver.search_branching`: `{best_run.get('solver.search_branching', 'N/A')}`
    * `solver.stop_after_first_solution`: `{best_run.get('solver.stop_after_first_solution', 'N/A')}`
    * `solver.cp_model_presolve`: `{best_run.get('solver.cp_model_presolve', 'N/A')}`
    * `solver.linearization_level`: `{best_run.get('solver.linearization_level', 'N/A')}`

### Top 5 Runs (Aggregated across all datasets)
{report_table}
"""
    try:
        with RESULTS_DOC_PATH.open("a", encoding="utf-8") as f:
            f.write(report_content)
        logging.info(f"Successfully appended results to {RESULTS_DOC_PATH}")
    except Exception as e:
        logging.error(f"Failed to write to {RESULTS_DOC_PATH}: {e}")
        # Don't fail the whole script, just log the error
        pass


def main() -> int:
    """Main execution function."""
    setup_logging()

    try:
        # --- 1. Load Criteria ---
        logging.info(f"Loading objectives from {TUNE_GRID_PATH}")
        tune_cfg = load_yaml_config(TUNE_GRID_PATH)

        obj_primary = tune_cfg.get("objective_primary", "utilization")
        obj_primary_goal = tune_cfg.get("objective_primary_goal", "maximize")
        obj_secondary = tune_cfg.get("objective_secondary", "runtime_sec")
        obj_secondary_goal = tune_cfg.get("objective_secondary_goal", "minimize")

        logging.info(f"Primary Objective: {obj_primary} ({obj_primary_goal})")
        logging.info(f"Secondary Objective: {obj_secondary} ({obj_secondary_goal})")

        # --- 2. Load Data ---
        logging.info(f"Loading results data from {RESULTS_CSV_PATH}")
        if not RESULTS_CSV_PATH.exists():
            logging.error(
                f"Results file not found: {RESULTS_CSV_PATH}\n"
                "Please run 'scripts/tune.py' first."
            )
            return 1

        df = pd.read_csv(RESULTS_CSV_PATH)
        logging.info(f"Loaded {len(df)} total run results.")

        # --- 3. Filter ---
        valid_statuses = ["OPTIMAL", "FEASIBLE"]
        df_valid = df[df["status"].isin(valid_statuses)].copy()

        if df_valid.empty:
            logging.error(
                "No 'OPTIMAL' or 'FEASIBLE' runs found in results. " "Cannot select a best config."
            )
            return 1

        logging.info(f"Found {len(df_valid)} valid (FEASIBLE/OPTIMAL) runs.")

        # --- 4. Sort ---
        # Determine ascending/descending
        sort_primary_asc = obj_primary_goal == "minimize"
        sort_secondary_asc = obj_secondary_goal == "minimize"

        df_sorted = df_valid.sort_values(
            by=[obj_primary, obj_secondary],
            ascending=[sort_primary_asc, sort_secondary_asc],
        )

        # --- 5. Select Best ---
        best_run = df_sorted.iloc[0]
        logging.info(f"Best run selected (ID: {best_run['run_id']}):")
        logging.info(
            f"  -> {obj_primary}: {best_run[obj_primary]:.4f}, "
            f"{obj_secondary}: {best_run[obj_secondary]:.2f} sec"
        )

        # --- 6. Extract Parameters ---
        # Parameters are all columns that are NOT metrics
        param_keys = [col for col in df_sorted.columns if col not in METRIC_COLUMNS]
        best_params = best_run[param_keys].to_dict()

        # --- 7. Generate Config ---
        logging.info(f"Loading base config from {BASE_CONFIG_PATH}")
        base_cfg = load_yaml_config(BASE_CONFIG_PATH)

        # --- 8. Save Config ---
        save_best_config(base_cfg, best_params)

        # --- 9. Report (DoD) ---
        df_top5 = df_sorted.head(5).copy()
        update_markdown_report(df_top5, best_run)

    except Exception as e:
        logging.error(f"Critical error in select_best_config: {e}", exc_info=True)
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())
